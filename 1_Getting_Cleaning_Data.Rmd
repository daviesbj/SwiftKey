---
title: "Getting and Cleaning the Data"
author: "Brian Davies"
date: "09 March 2015"
output: html_document
---

# Basic setup

```{r basic_setup}
set.seed(773230)
```

# Downloading the Data

Just do the obvious thing to start with.

```{r download_and_unzip_data, cache = TRUE }
sourceURL = 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
download.file( url = sourceURL, destfile = basename( sourceURL), method = 'curl' )
unzip('Coursera-SwiftKey.zip')
```

# Loading the Data

I've got a pretty good computer, so I'm just gonna load the lot

```{r load_all_the_text, cache = TRUE }
blogsRaw <- readLines( '~/SwiftKey/final/en_US/en_US.blogs.txt' )
newsRaw <- readLines( '~/SwiftKey/final/en_US/en_US.news.txt' )
twitRaw <- readLines( '~/SwiftKey/final/en_US/en_US.twitter.txt' )
```

Resampling the data. Lets take 1% of each lot.

```{r subsample_raw_data, cache = TRUE}
textSample <- c(
  sample( blogsRaw, size = floor( length( blogsRaw ) / 100 ), replace = FALSE ),
  sample( newsRaw, size = floor( length( newsRaw ) / 100 ), replace = FALSE ),
  sample( twitRaw, size = floor( length( twitRaw ) / 100 ), replace = FALSE )
)
textSample <- sample( textSample, size = length( textSample ), replace = FALSE )
```

And change it to words

```{r change_to_words, cache = TRUE}
textSample2 <- tolower( textSample )
# Letters & apostrophes only
textSample3 <- gsub( "[^abcdefghijklmnopqrstuvwxyz' ]", ' ', textSample2, perl = TRUE )
# Manage unwanted spaces
textSample4 <- gsub( ' +', ' ', textSample3, perl = TRUE )
textSample5 <- gsub( '^ ', '', textSample4, perl = TRUE )
textSample6 <- gsub( ' $', '', textSample5, perl = TRUE )
```

I'm going to change it to a list of arrays of tokens

```{r change_to_token_list, cache = TRUE}
textList <- as.list( textSample6 )
textList <- sapply( textList, function(x) strsplit( x[1], ' ', fixed = TRUE ) )
```

Some code to answer the questions in the first quiz.

```{r Quiz_Questions, cache = TRUE }
# Q1. The en_US.blogs.txt file is how many megabytes?
file.info( 'final/en_US//en_US.blogs.txt')$size/(2**20)
# Q2. The en_US.twitter.txt has how many lines of text?
length( twitRaw )
# Q3. What is the length of the longest line seen in any of the three en_US data sets?
max(nchar(blogsRaw))
max(nchar(newsRaw))
max(nchar(twitRaw))
# Q4. In the en_US twitter data set, if you divide the number of lines where the word "love" (all lowercase) occurs by the number of lines the word "hate" (all lowercase) occurs, about what do you get?
sum( grepl( '\\blove\\b', twitRaw, perl = TRUE )) / sum( grepl( '\\bhate\\b', twitRaw, perl = TRUE ))
# Q5. The one tweet in the en_US twitter data set that matches the word "biostats" says what?
twitRaw[ grepl( '\\bbiostats\\b', twitRaw, perl = TRUE, ignore.case = TRUE ) ]
# Q6. How many tweets have the exact characters "A computer once beat me at chess, but it was no match for me at kickboxing". (I.e. the line matches those characters exactly.)
sum( twitRaw == "A computer once beat me at chess, but it was no match for me at kickboxing" )
```

How to filter profanity? I'm going to remove every token that contains an embedded word that corresponds
to one of my profanities. Need to experiment with this.

```{r deprofanizer_function, cache = TRUE}
NoProf <- function( someWords, badWordsJoined ) someWords[ ! grepl( badWordsJoined, someWords, perl = TRUE ) ]
NoProf( c( 'apple', 'fox', 'pear', 'badgering', 'stoats', 'cherry', 'foxy', 'arcticfox' ),
        paste0( c( 'fox', 'badger' ), collapse = '|' ))
NoProf( c( 'apple', 'fox', 'pear', 'badgering', 'stoats', 'cherry', 'foxy', 'arcticfox' ),
        paste0( c( '^fox$', 'badger' ), collapse = '|' ))
profanities <- readLines( '~/SwiftKey/profanity.txt' )
profPatterns <- paste0( profanities, collapse = '|' )
textListClean <- sapply( textList, function(x) NoProf( x, profPatterns ) )
```

That kinda-sorta seems to work, so now to do the whole bang-shoot
